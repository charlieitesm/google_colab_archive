{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_Embedding_Tokenizer_Clase.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1lpo5_6YfUG",
        "colab_type": "text"
      },
      "source": [
        "#Word Embedding - Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJdhumgV8CkJ",
        "colab_type": "text"
      },
      "source": [
        "Embedding(input_dim, output_dim, input_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAq1w9_q8UXQ",
        "colab_type": "text"
      },
      "source": [
        "input_dim = Total de palabras en el vocabulario/diccionario. Debe estar indexado, i.e., 1, 2, 3,..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soEulkgY8gaQ",
        "colab_type": "text"
      },
      "source": [
        "output_dim = Dimensión del espacio vectorial en el cual serán embebidas cada una de las palabras del vocabulario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avn3EqcU9KZY",
        "colab_type": "text"
      },
      "source": [
        "input_length = Longitud de la secuencia de palabras de entrada de cada documento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpAP3Z6r95Nf",
        "colab_type": "text"
      },
      "source": [
        "Recuerda que el proceso de Word-Embedding se puede ver como una proyección: un vector de entrada de un espacio dicretizado de palabras, es PROYECTADO a un espacio continuo de vectores de dimensión fija. Dicha proyección de una palabra dada \"w\", está basada en todas aquellas palabras que rodean a \"w\" en todos los documentos en que aparece.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIlfVylS-2o1",
        "colab_type": "text"
      },
      "source": [
        "#Ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jpcx6KC9DpLW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8c39c71-f416-44bd-c0b7-bc3ff7f51420"
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer   \n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "\n",
        "from keras.layers.embeddings import Embedding\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmJJUq1v-5cK",
        "colab_type": "text"
      },
      "source": [
        "Definamos 10 documentos, 5 positivos y 5 negativos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRwcUfvh706X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "docs = [ 'Muy bien hecho lo hecho',\n",
        "        'Excelente trabajo muchacho',\n",
        "        'Sigue como hasta ahora',\n",
        "        'Lo hiciste muy bien',\n",
        "        'Que siga el buen ánimo',\n",
        "        'Pésimo trabajo',\n",
        "        'Que mal desempeño',\n",
        "        'Muy mal hecho',\n",
        "        'Que trabajo tan pobre',\n",
        "        'No está bien']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huzORawIBftU",
        "colab_type": "code",
        "outputId": "e0a7670a-cab1-4d9d-b97d-bfb2848016ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "for d in docs:\n",
        "  print(d)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Muy bien hecho lo hecho\n",
            "Excelente trabajo muchacho\n",
            "Sigue como hasta ahora\n",
            "Lo hiciste muy bien\n",
            "Que siga el buen ánimo\n",
            "Pésimo trabajo\n",
            "Que mal desempeño\n",
            "Muy mal hecho\n",
            "Que trabajo tan pobre\n",
            "No está bien\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhSc1VC1AMzr",
        "colab_type": "text"
      },
      "source": [
        "Definamos las etiquetas de ambas clases, positivas y negativas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlLNYD2wAR8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = np.array([1,1,1,1,1, 0,0,0,0,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV31MsqM7FbL",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHfZ3TlLBIdT",
        "colab_type": "text"
      },
      "source": [
        "Se requiere que las palabras/tokens de cada documento estén indexados. Esto lo podemos hacer con la clase Tokenizer de Keras. Puedes definir el número de variables, num_words, mayor al total de palabras de vocabulario si no sabes cuántas puedes tener, es mejor tener un valor mayor para evitar colisiones.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn0tFHy0gIwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_words = 40"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCzOkmVB7K-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = Tokenizer(num_words=n_words, filters='!\"#$%&()*+,-./:;<=>?...', lower=True, split=' ', oov_token='#')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SDrM9UvaOXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens.fit_on_texts(docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj-8wqT0FLiG",
        "colab_type": "text"
      },
      "source": [
        "Veamos el diccionario generado de cada palabra con su índice:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F049Hh5T99Gv",
        "colab_type": "code",
        "outputId": "c3afaa5a-cfa7-4a87-f19e-71a7cd4816fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(tokens.word_index)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'#': 1, 'muy': 2, 'bien': 3, 'hecho': 4, 'trabajo': 5, 'que': 6, 'lo': 7, 'mal': 8, 'excelente': 9, 'muchacho': 10, 'sigue': 11, 'como': 12, 'hasta': 13, 'ahora': 14, 'hiciste': 15, 'siga': 16, 'el': 17, 'buen': 18, 'ánimo': 19, 'pésimo': 20, 'desempeño': 21, 'tan': 22, 'pobre': 23, 'no': 24, 'está': 25}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjBc3fzjLw-a",
        "colab_type": "text"
      },
      "source": [
        "Podemos obtener el índice de una palabra del vocabulario:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0HaygqpLj8R",
        "colab_type": "code",
        "outputId": "694c17b0-c364-419e-e345-08ebb2cc43c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokens.word_index['siga']"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xie075_nMBXG",
        "colab_type": "text"
      },
      "source": [
        "O bien, obtener la palabra de un índice particular:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-c-gpRDMIP8",
        "colab_type": "code",
        "outputId": "06530692-7002-4680-e0b3-679676f315a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokens.index_word[9]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'excelente'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks3cy5Rxar8B",
        "colab_type": "text"
      },
      "source": [
        "Indexamos cada documento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2GcTHUa8KrV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_docs= tokens.texts_to_sequences(docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXS8IHpa8boL",
        "colab_type": "code",
        "outputId": "81faf48b-2aeb-423a-b9e5-e8e5053e2bb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(encoded_docs)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2, 3, 4, 7, 4], [9, 5, 10], [11, 12, 13, 14], [7, 15, 2, 3], [6, 16, 17, 18, 19], [20, 5], [6, 8, 21], [2, 8, 4], [6, 5, 22, 23], [24, 25, 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot1hCg5pa0ou",
        "colab_type": "text"
      },
      "source": [
        "Verificamos el total de palabras en nuestro vocabulario:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EY7beGPZF7rJ",
        "colab_type": "code",
        "outputId": "37aa7a21-dcad-4cc0-a5ff-5f085e4c9655",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab_size = len(tokens.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXeMs2GZHZRp",
        "colab_type": "text"
      },
      "source": [
        "Generamos ahora las secuencias de longitud determinada de cada documento :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qt1jo70UIe2n",
        "colab_type": "text"
      },
      "source": [
        "Se completa con ceros cuando es menor la longitud del documento, o bien, se trunca cuando es mayor la longitud del documento. En este ejemplo usemos una longitud de 6:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vgkH13VIdnP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences_length = 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_D4rtZxxuSl",
        "colab_type": "text"
      },
      "source": [
        "El argumento padding={pre, post}, completa con ceros antes o después de cada secuencia:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy95HAFrI3mq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "padded_docs = pad_sequences(encoded_docs, maxlen=sequences_length, padding='post')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgdL6cxZJadg",
        "colab_type": "code",
        "outputId": "efe5c1a4-5b77-43ca-ec85-311f6501db78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "print(padded_docs)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 2  3  4  7  4  0]\n",
            " [ 9  5 10  0  0  0]\n",
            " [11 12 13 14  0  0]\n",
            " [ 7 15  2  3  0  0]\n",
            " [ 6 16 17 18 19  0]\n",
            " [20  5  0  0  0  0]\n",
            " [ 6  8 21  0  0  0]\n",
            " [ 2  8  4  0  0  0]\n",
            " [ 6  5 22 23  0  0]\n",
            " [24 25  3  0  0  0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuzFbAv0zpjk",
        "colab_type": "text"
      },
      "source": [
        "#Modelo:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36vOQ-ZZcORh",
        "colab_type": "text"
      },
      "source": [
        "En este ejemplo generemos vectores de cada palabra continuos de dimensión 4:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBZMo2Lny1fF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedded_vector_sz=4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBdmkUKszoby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedded_vector_sz, input_length=sequences_length))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa_Mod3p1L-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZJO2INw3mgj",
        "colab_type": "text"
      },
      "source": [
        "Observemos en el resumen del modelo que el tamaño de la matriz Embedding W es de 6x4, i.e., entra un vector/sequence \"u\" de longitud 6 y sale uno de longitud 4:  u^t*W. Sin embargo, contiene a su vez la información de la transformación de los 26 tokens de nuestro vocabulario al vector continuo 4 dimensional."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X9jis6c1d7E",
        "colab_type": "code",
        "outputId": "78c5a626-2e0c-4299-d741-9c3894690fd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 6, 4)              104       \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 24)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 25        \n",
            "=================================================================\n",
            "Total params: 129\n",
            "Trainable params: 129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZTELT6o121o",
        "colab_type": "code",
        "outputId": "95baa4cf-64c8-46e3-899e-91e8605c5c6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "model.fit(padded_docs, labels, epochs=40, verbose=0)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f2b1e042400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0kl3AQF2Kwg",
        "colab_type": "code",
        "outputId": "f92e4218-bd5e-403e-d949-1fc677e083db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "loss, accuracy = model.evaluate(padded_docs, labels, verbose=1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r10/10 [==============================] - 0s 2ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EID2ckz42WbB",
        "colab_type": "code",
        "outputId": "c1566a2c-1422-4ed5-a91f-84902187af55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print('Accuracy: %f' % (accuracy*100))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 100.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h92tkQNdYFE",
        "colab_type": "text"
      },
      "source": [
        "#Word Embedding Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0CvMvFoBCaC",
        "colab_type": "text"
      },
      "source": [
        "Para recuperar la matriz WordEmbeddig debemos utilizar únicamente la primera capa del modelo anterior:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWhZ1dHwCNbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "matrixEmbedding = Model(inputs=model.input, outputs=model.get_layer('embedding_1').output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv8DJcgyEA33",
        "colab_type": "code",
        "outputId": "2990a64d-dbea-4d93-8d47-249374eecdad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "matrixEmbedding.output_shape"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, 6, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD-tjeGsEUVK",
        "colab_type": "text"
      },
      "source": [
        "Y a su vez obtener los vectores continuos 4-dimensionales para cada una de los 26 tokens de nuestro vocabulario:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4v4-g5sEEeba",
        "colab_type": "code",
        "outputId": "2be2f95c-83cf-4db3-cfb8-300734811b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 509
        }
      },
      "source": [
        "matrixEmbedding.weights"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'embedding_1/embeddings:0' shape=(26, 4) dtype=float32, numpy=\n",
              " array([[-0.04464644, -0.05516564,  0.00996508, -0.02029327],\n",
              "        [-0.00343312,  0.0278131 , -0.00697067, -0.04113562],\n",
              "        [-0.0008171 ,  0.08894813,  0.07018009, -0.00795872],\n",
              "        [ 0.080949  ,  0.01199691, -0.03984289, -0.0675302 ],\n",
              "        [ 0.04170085,  0.06890322,  0.07722773,  0.01145003],\n",
              "        [-0.00337593,  0.02143945, -0.006995  , -0.07491899],\n",
              "        [ 0.01655192, -0.07871365,  0.08654396, -0.07193401],\n",
              "        [ 0.01155774,  0.06323144,  0.00806769,  0.06426584],\n",
              "        [-0.06300905,  0.02952315,  0.01216486, -0.00665407],\n",
              "        [-0.00523054,  0.07882133, -0.00823868,  0.05557589],\n",
              "        [ 0.00184952,  0.08266567,  0.06707029, -0.00067254],\n",
              "        [-0.08322014,  0.07135184, -0.04240251,  0.0244072 ],\n",
              "        [ 0.04049359, -0.00655989, -0.03818602,  0.0205704 ],\n",
              "        [-0.03724208,  0.0182146 , -0.00022752,  0.0137044 ],\n",
              "        [ 0.0563774 ,  0.05503864, -0.02534409, -0.01579331],\n",
              "        [ 0.04409276, -0.0093971 , -0.05317629,  0.05370354],\n",
              "        [ 0.02891952, -0.02650414, -0.00544661,  0.01958781],\n",
              "        [-0.02530083,  0.02863313,  0.04766979,  0.07827707],\n",
              "        [ 0.03117838,  0.07922836, -0.00703129, -0.04722486],\n",
              "        [ 0.06193097,  0.04343918,  0.0418629 ,  0.08814865],\n",
              "        [ 0.03504785, -0.0685937 ,  0.0485291 , -0.03329598],\n",
              "        [ 0.02943775, -0.00026743, -0.02910294, -0.01765333],\n",
              "        [-0.01113987, -0.07700041, -0.05693812, -0.06681406],\n",
              "        [ 0.00071503, -0.01148281, -0.00960862,  0.07539966],\n",
              "        [ 0.06047745, -0.08564305,  0.03386466, -0.07442895],\n",
              "        [-0.06239035,  0.03488438,  0.00597591, -0.07813485]],\n",
              "       dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JehPYmGFeoEM",
        "colab_type": "text"
      },
      "source": [
        "El oreden está dado en nuestro diccionario tiene 25 tokens, más el primer índice 0 que no está asociado a ningún token (por eso la longitud 26):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oGMbKkjesbh",
        "colab_type": "code",
        "outputId": "f8c76cd3-75b1-4701-ac30-446659b869ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(tokens.word_index)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'#': 1, 'muy': 2, 'bien': 3, 'hecho': 4, 'trabajo': 5, 'que': 6, 'lo': 7, 'mal': 8, 'excelente': 9, 'muchacho': 10, 'sigue': 11, 'como': 12, 'hasta': 13, 'ahora': 14, 'hiciste': 15, 'siga': 16, 'el': 17, 'buen': 18, 'ánimo': 19, 'pésimo': 20, 'desempeño': 21, 'tan': 22, 'pobre': 23, 'no': 24, 'está': 25}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FjuGLdwz1ud",
        "colab_type": "text"
      },
      "source": [
        "Recuerda que en realidad en este ejemplo son 24 palabras, más el símbolo \"#\" que es el índice 1. Y el índice 0 no se usa. Por ello es de longitud 26."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TjSn5hbjpVZ",
        "colab_type": "text"
      },
      "source": [
        "Podemos ahora obtener los vectores densos de una secuencia de palabras de longitud 6 o de alguna en particular:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5M-TbuiixHT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cda91b2c-1a57-47c3-d80e-a2aff09b7778"
      },
      "source": [
        "matrixEmbedding.predict(x=np.array([[ 9,  0,  0,  0,  0,  0 ]]))[0][0]  # excelente(9)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.00523054,  0.07882133, -0.00823868,  0.05557589], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHzd_sQYkcuv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "d68c232a-0c67-4738-86e8-78069890e2d7"
      },
      "source": [
        "matrixEmbedding.predict(x=np.array([[ 18,  5,  0,  0,  0,  0 ]]))[0]  # buen(18) trabajo(5)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.03117838,  0.07922836, -0.00703129, -0.04722486],\n",
              "       [-0.00337593,  0.02143945, -0.006995  , -0.07491899],\n",
              "       [-0.04464644, -0.05516564,  0.00996508, -0.02029327],\n",
              "       [-0.04464644, -0.05516564,  0.00996508, -0.02029327],\n",
              "       [-0.04464644, -0.05516564,  0.00996508, -0.02029327],\n",
              "       [-0.04464644, -0.05516564,  0.00996508, -0.02029327]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1uGBY8_tRb0",
        "colab_type": "text"
      },
      "source": [
        "Es difícil que con tan pocos documentos y vocabulario se alcance a encontrar alguna similaridad entre palabras, pero veamos las siguientes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmL2q9k8sQ7O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "9b3dc943-0bf5-4d41-b2da-ae0634e158d4"
      },
      "source": [
        "matrixEmbedding.predict(x=np.array([[ 3,  18,  20,  24,  0,  0 ]]))[0][0:4]   # bien(3), buen(18), pésimo(20), no(24)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.080949  ,  0.01199691, -0.03984289, -0.0675302 ],\n",
              "       [ 0.03117838,  0.07922836, -0.00703129, -0.04722486],\n",
              "       [ 0.03504785, -0.0685937 ,  0.0485291 , -0.03329598],\n",
              "       [ 0.06047745, -0.08564305,  0.03386466, -0.07442895]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwD3IYQ-65Vi",
        "colab_type": "text"
      },
      "source": [
        "#Podemos guardar tanto el modelo como la matriz Word-Embedding:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dVz-UOoUw2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model.save('mimodelo.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bYHiotdU4Ew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#matrixEmbedding.save_weights('mispesos.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}