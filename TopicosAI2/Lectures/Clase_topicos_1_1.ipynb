{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase_topicos_1_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28rA8XO4kzti"
      },
      "source": [
        "#Tokenización o segmentación por tokens\n",
        "\n",
        "### Se puede separar un texto en oraciones, palabras, caracteres o algunos otros criterios.\n",
        "\n",
        "### Usaremos NLTK en los siguientes ejemplos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "375Ze6Kt2_Cx"
      },
      "source": [
        "# Para instalarlo por primera vez:\r\n",
        "#!pip install nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dghIdSpVlBlG"
      },
      "source": [
        "import nltk  # Python Natural Language Toolkit:  https://www.nltk.org/ \r\n",
        "nltk.download('punkt')      #  Punkt Sentence Tokenizer:   https://www.kite.com/python/docs/nltk.punkt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhOEmtjy3atA"
      },
      "source": [
        "#nltk.download()  # Cuando lo deees, puedes seleccionar el o los paquetes particulares que desees instalar."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyF-bBp65_G9"
      },
      "source": [
        "f = open(\"arreola_guardagujas.txt\", \"r\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGS3LhPm6IMn"
      },
      "source": [
        "cuento = f.read()\r\n",
        "cuento   # este es un string que contiene todo el documento (cuento) de Arreola."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKMIU5pR7aqz",
        "outputId": "5872f6ca-97df-4b0c-f638-80270d49efe3"
      },
      "source": [
        "len(cuento)  # Total de caracteres del cuento/documento."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14059"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7AyVC1I7wwL"
      },
      "source": [
        "###**Tokenización por enunciados:** \r\n",
        "\r\n",
        "###Por default NLTK tokeniza texto en inglés, pero también puede tokenizar otros idiomas como Español."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcsAGzIk8LZe"
      },
      "source": [
        "# Podemos tokenizar nuestro corpus en oraciones usando el modelo predeterminado en Español ya integrado:\r\n",
        "sent = nltk.sent_tokenize(text=cuento, language='spanish')   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DF9OFtgo8Rez"
      },
      "source": [
        "sent[0:10]    # observa que la separación de las oraciones predeterminada no es por salto de página \"\\n\"."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q38S5WU8yVr"
      },
      "source": [
        "senten = nltk.sent_tokenize(text=cuento, language='french')    # german, italian, french, portuguese...\r\n",
        "senten[0:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmwHBYBVBDYK"
      },
      "source": [
        "Veamos si usando los modelos pre-entrenados con otros idiomas diferentes al español resulta \"exactamente\" la misma división de oraciones:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pINncFB4BPAl"
      },
      "source": [
        "print(sent==senten)   # con algunos idiomas diferentes al del documento se pueden obtener diferentes tokenizaciones."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEijloWH0ama"
      },
      "source": [
        "### En caso de tener un modelo personal pre-entrenado, lo podríamos utilizar también.\r\n",
        "\r\n",
        "###El poder entrenar corpus con idiomas particulares ayuda a identificar mejor las partes de las oraciones, estructuras gramaticales, palabras o caracteres usuales con las que inicia o termina una oración, palbras que por lo general aparecen juntas, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1UztdJf5wx0"
      },
      "source": [
        "spanish_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/spanish.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COlnJ2YN-Axy"
      },
      "source": [
        "ss = spanish_tokenizer.tokenize(cuento.strip())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAD09f5y_bce"
      },
      "source": [
        "### **Tokenización por palabras:**\r\n",
        "\r\n",
        "Una vez que tienes cada oración, podrás ahora separarlas por \"palabras\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aaUpMxR_ACR"
      },
      "source": [
        "w = nltk.word_tokenize(ss[7], language='spanish')\r\n",
        "print(w)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4n3dCeolHTwg"
      },
      "source": [
        "### Separemos todo el cuento de Arreola en palabras:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-cGgmkdHfRD"
      },
      "source": [
        "sent = nltk.sent_tokenize(text=cuento, language='spanish')   \r\n",
        "tokens_words = [nltk.word_tokenize(s) for s in sent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hskbojUVD__h"
      },
      "source": [
        "###Veamos como se lleva a cabo la separación de algunas contracciones en inglés: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBAr4Nma-_-A"
      },
      "source": [
        "r = \"haven't I'll ain't can't everyone's o'clock ol' 'tis\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-uJC_Mw-_6H"
      },
      "source": [
        "print(nltk.word_tokenize(r, language='english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0XfyLCCZc3w"
      },
      "source": [
        "### **Usando el método split() de string** \r\n",
        "\r\n",
        "Cuando cargas o generas un documento separado por renglones de strings, también podrás seguir tokenizándolo bajo diferentes criterios. \r\n",
        "\r\n",
        "Veamos algunos ejemolos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fPgLaj_ZvKZ"
      },
      "source": [
        "sent  # tenemos una lista de enunciados (strings) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLcIeO4GJhZi"
      },
      "source": [
        "tks1 = []             \r\n",
        "for linea in sent:\r\n",
        "  x = str(linea).split('\\n')   # separando por saltos de línea \"\\n\"\r\n",
        "  tks1.append(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycbc9L4IdeJH"
      },
      "source": [
        "tks2 = []             \r\n",
        "for linea in sent:\r\n",
        "  x = str(linea).split(' ')   # separando por espacios en blanco ... en dado caso puedes separar por \",\", \".\", etc...\r\n",
        "  tks2.append(x)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyOqepBPd2K_"
      },
      "source": [
        "tks3 = []             \r\n",
        "for linea in sent:\r\n",
        "  x = str(linea).split()   # en este caso separa por espacios en blanco y saltos de línea \"\\n\"\r\n",
        "  tks3.append(x)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oynm3BcJhEYm"
      },
      "source": [
        "print(len(sent))\r\n",
        "print(len(tks1))\r\n",
        "print(len(tks2))\r\n",
        "print(len(tks3))\r\n",
        "print(len(tks1[0]))\r\n",
        "print(len(tks2[0]))\r\n",
        "print(len(tks3[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HhxIIITiVB1"
      },
      "source": [
        "### Podemos concatenar también todos los enunciados para obtener un solo documento formado por una lista de tokens (palabras):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2JBZMeChy96"
      },
      "source": [
        "u=[]\r\n",
        "for t in tks3:   # para el caso de una lista de enunciados formado por strings de palabras\r\n",
        "  u += t\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}