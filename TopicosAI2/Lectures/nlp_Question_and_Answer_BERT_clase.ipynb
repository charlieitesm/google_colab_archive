{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_Question_and_Answer_BERT_clase.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISSJbZCuqlXg"
      },
      "source": [
        "#Modelo BERT pre-entrenado para el problema de Preguntas y Respuestas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D99_ikqrrYky"
      },
      "source": [
        "Nos basaremos en la librería HuggingFace que nos proporciona modelos Transformer pre-entrenados:\n",
        "\n",
        "https://huggingface.co/\n",
        "\n",
        "https://github.com/huggingface/transformers/\n",
        "\n",
        "\n",
        "En particular este ejemplo está basado en el pequeño ejemplo mostrado para el modelo pre-entrenado TFBertForQuestionAnswering, que es el modelo BERT_Large pre-entrenado (fine-tuning) para problemas de preguntas y respuestas:\n",
        "\n",
        "https://huggingface.co/transformers/model_doc/bert.html?highlight=bertforquestionanswering#tfbertforquestionanswering \n",
        "\n",
        "https://huggingface.co/transformers/pretrained_models.html\n",
        "\n",
        "\n",
        "Usaremos además el modelo siguiente pre-entrenado (fine-tuning) con la base de datos SQuad::\n",
        "\n",
        "https://huggingface.co/bert-large-uncased-whole-word-masking-finetuned-squad\n",
        "\n",
        "https://rajpurkar.github.io/SQuAD-explorer/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQl0MMrOGIup"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w0JYs0cDRsx"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI7FT9-ZxLOb"
      },
      "source": [
        "Descargemos el modelo. Estrictamente no se descarga en nuestro disco, sino que el modelo está en algún lugar de la web y que nos proporciona HuggingFace.\n",
        "\n",
        "Observa que el modelo es de 1.34GB."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mnv95sX-U9K"
      },
      "source": [
        "from transformers import TFBertForQuestionAnswering\n",
        "model = TFBertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze9lR2uH0Na1"
      },
      "source": [
        "Cargamos el tokenizer con su vocabulario en el caso uncased:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFQ5f7gv-RBH"
      },
      "source": [
        "from transformers import BertTokenizer \n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmFi5cCE0c6l"
      },
      "source": [
        "tokenizer.vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I__1ubvcZYow"
      },
      "source": [
        "## Utilicemos el modelo pre-entrenado para realizar alguna preguntas, dado un texto.\n",
        "\n",
        "Para este ejemplo usarmos algunos párrafos de Wikipedia sobre Stephen Hawking:\n",
        "\n",
        "https://en.wikipedia.org/wiki/Stephen_Hawking\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx6rG4kSIIWk"
      },
      "source": [
        "Hawking was born in Oxford into a family of doctors. He began his university education at University College, Oxford, in October 1959 at the age of 17, where he received a first-class BA (Hons.) degree in physics. He began his graduate work at Trinity Hall, Cambridge, in October 1962, where he obtained his PhD degree in applied mathematics and theoretical physics, specialising in general relativity and cosmology in March 1966. \n",
        "\n",
        "In 1963, Hawking was diagnosed with an early-onset slow-progressing form of motor neurone disease that gradually paralysed him over the decades.[20][21] After the loss of his speech, he communicated through a speech-generating device initially through use of a handheld switch, and eventually by using a single cheek muscle. Hawking achieved commercial success with several works of popular science in which he discussed his theories and cosmology in general. His book A Brief History of Time appeared on the Sunday Times bestseller list for a record-breaking 237 weeks. Hawking was a Fellow of the Royal Society, a lifetime member of the Pontifical Academy of Sciences, and a recipient of the Presidential Medal of Freedom, the highest civilian award in the United States. In 2002, Hawking was ranked number 25 in the BBC's poll of the 100 Greatest Britons. He died on 14 March 2018 at the age of 76, after living with motor neurone disease for more than 50 years."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtcNgtFOC1Hl"
      },
      "source": [
        "import textwrap\n",
        "from textwrap import wrap\n",
        "wrapper = textwrap.TextWrapper(width=70) # ancho del texto a despegar.\n",
        "#texto = \"Hawking was born in Oxford into a family of doctors. He began his university education at University College, Oxford, in October 1959 at the age of 17, where he received a first-class BA (Hons.) degree in physics. He began his graduate work at Trinity Hall, Cambridge, in October 1962, where he obtained his PhD degree in applied mathematics and theoretical physics, specialising in general relativity and cosmology in March 1966. In 1963, Hawking was diagnosed with an early-onset slow-progressing form of motor neurone disease that gradually paralysed him over the decades.[20][21] After the loss of his speech, he communicated through a speech-generating device initially through use of a handheld switch, and eventually by using a single cheek muscle. Hawking achieved commercial success with several works of popular science in which he discussed his theories and cosmology in general. His book A Brief History of Time appeared on the Sunday Times bestseller list for a record-breaking 237 weeks. Hawking was a Fellow of the Royal Society, a lifetime member of the Pontifical Academy of Sciences, and a recipient of the Presidential Medal of Freedom, the highest civilian award in the United States. In 2002, Hawking was ranked number 25 in the BBC's poll of the 100 Greatest Britons. He died on 14 March 2018 at the age of 76, after living with motor neurone disease for more than 50 years.\"\n",
        "texto = \"In 1963, Hawking was diagnosed with an early-onset slow-progressing form of motor neurone disease that gradually paralysed him over the decades.[20][21] After the loss of his speech, he communicated through a speech-generating device initially through use of a handheld switch, and eventually by using a single cheek muscle. Hawking achieved commercial success with several works of popular science in which he discussed his theories and cosmology in general. His book A Brief History of Time appeared on the Sunday Times bestseller list for a record-breaking 237 weeks. Hawking was a Fellow of the Royal Society, a lifetime member of the Pontifical Academy of Sciences, and a recipient of the Presidential Medal of Freedom, the highest civilian award in the United States.\"\n",
        "print(wrapper.fill(texto)) #  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lux8K_UDKf9"
      },
      "source": [
        "question = \"what disease did Hawking have?\"\n",
        "\n",
        "#question = \"How did Hawking talk?\"\n",
        "#question = \"What is the title of his best-seller?\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYoX33CfKGsr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "ff662262-0ede-4f02-c224-eab8fd01b4a5"
      },
      "source": [
        "# Apply the tokenizer to the input text, treating them as a text-pair.\n",
        "input_ids = tokenizer.encode(question, texto)\n",
        "print('The input has a total of {:} tokens.'.format(len(input_ids)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The input has a total of 165 tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iow838yPNDTv"
      },
      "source": [
        "# BERT only needs the token IDs, but for the purpose of inspecting the \n",
        "# tokenizer's behavior, let's also get the token strings and display them.\n",
        "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "# For each token and its id...\n",
        "for token, id in zip(tokens, input_ids):\n",
        "    \n",
        "    # If this is the [SEP] token, add some space around it to make it stand out.\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')\n",
        "    \n",
        "    # Print the token string and its ID in two columns.\n",
        "    print('{:<12} {:>6,}'.format(token, id))\n",
        "\n",
        "    if id == tokenizer.sep_token_id:\n",
        "        print('')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ7PRx2dKqFN"
      },
      "source": [
        "# Search the input_ids for the first instance of the `[SEP]` token.\n",
        "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "\n",
        "# The number of segment A tokens includes the [SEP] token istelf.\n",
        "num_seg_a = sep_index + 1\n",
        "\n",
        "# The remainder are segment B.\n",
        "num_seg_b = len(input_ids) - num_seg_a\n",
        "\n",
        "# Construct the list of 0s and 1s.\n",
        "segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "\n",
        "# There should be a segment_id for every input token.\n",
        "assert len(segment_ids) == len(input_ids)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AymIov1_RrkU"
      },
      "source": [
        "tf.convert_to_tensor([input_ids], np.int32) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQiKr6Aw-YTg"
      },
      "source": [
        "outputs = model(tf.convert_to_tensor([input_ids], dtype=np.int32),\n",
        "                token_type_ids = tf.convert_to_tensor([segment_ids], dtype=np.int32) , \n",
        "                return_dict=True) \n",
        "\n",
        "start_scores = outputs.start_logits\n",
        "end_scores = outputs.end_logits\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nag3SFRbK--t"
      },
      "source": [
        "#start_scores\n",
        "#end_scores"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22ICRRbhbL6h"
      },
      "source": [
        "start_scores   # EagerTensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLTyq9uzarOZ"
      },
      "source": [
        "start_scores.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqLsLZQ5eI2y"
      },
      "source": [
        "tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIkQK7DdbI7h"
      },
      "source": [
        "#tf.argmax(start_scores, axis=1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeUQ44hAJmn9"
      },
      "source": [
        "# Find the tokens with the highest `start` and `end` scores.\n",
        "\n",
        "answer_start = tf.argmax(start_scores, axis=1)\n",
        "answer_end = tf.argmax(end_scores, axis=1)\n",
        "\n",
        "answer = ' '.join(tokens[answer_start[0]:answer_end[0]+1])\n",
        "\n",
        "print('Answer: \"' + answer + '\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khral6HZXCuI"
      },
      "source": [
        "# Start with the first token.\n",
        "answer = tokens[answer_start[0]]\n",
        "\n",
        "# Select the remaining answer tokens and join them with whitespace.\n",
        "for i in range(answer_start[0] + 1, answer_end[0] + 1):\n",
        "    \n",
        "    # If it's a subword token, then recombine it with the previous token.\n",
        "    if tokens[i][0:2] == '##':\n",
        "        answer += tokens[i][2:]\n",
        "    \n",
        "    # Otherwise, add a space then the token.\n",
        "    else:\n",
        "        answer += ' ' + tokens[i]\n",
        "\n",
        "print('Answer: \"' + answer + '\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hh6nkIdXq-O"
      },
      "source": [
        "Veamos la salida probabilística de cada palabra"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkKFa73eJkPE"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "#sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (32,8)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C56AtMg2UBxN"
      },
      "source": [
        "s_scores = start_scores.numpy()\n",
        "e_scores = end_scores.numpy()\n",
        "\n",
        "# tokens identificados de manera única en el eje-x\n",
        "token_labels = []\n",
        "for (i, token) in enumerate(tokens):\n",
        "    token_labels.append('{:} - {:>2}'.format(token, i))\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6OAV1dL3-UB"
      },
      "source": [
        "ax = sns.barplot(x=token_labels, y=np.squeeze(s_scores), ci=None)\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "ax.grid(True)\n",
        "plt.title('Start_Word scores')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6tXEqIp-Tzou"
      },
      "source": [
        "ax = sns.barplot(x=token_labels, y=np.squeeze(e_scores), ci=None)\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
        "ax.grid(True)\n",
        "plt.title('End_Word scores')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}